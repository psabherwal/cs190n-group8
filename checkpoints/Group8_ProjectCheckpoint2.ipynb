{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e35778",
   "metadata": {},
   "source": [
    "# Term Project: Checkpoint #2\n",
    "\n",
    "**Group 8:** Palvi Sabherwal, Emily Thai, Hannah Shakouri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a37ec",
   "metadata": {},
   "source": [
    "## PROJECT UPDATE\n",
    "\n",
    "1. Provide an update on the status of your data collection. Have you been able to successfully collect and label a packet trace collected through netunicorn for your project? If yes, how many traces have you collected? Do you plan to scale up your data collection any further?\n",
    "\n",
    "**Currently, we have collected the data of five videos from each platform: YouTube, Vimeo, and Twitch. For YouTube and Vimeo, each video has a different duration (1 min, 2 min, 5 min, 10 min, and 15 min). For Twitch, each video is around 1 minute long. Our pipeline is setup to collect data from 15 videos (5 from each platform). In total, our pipeline has collected 15 packet traces (.pcap files). We are currently in the process of labeling and extracting the features of the packet traces we have collected so far. We are considering expanding our data collection (by 1-2 videos) for each platform. In the code below, we are using netUnicorn to collect the packet traces and preproccessing them to train our model.**\n",
    "\n",
    "2. Provide a list of features that you will extract from the packet traces for your model.\n",
    "\n",
    "**For our model, we plan on extracting these features from the packet traces:**\n",
    "- **Size:** of video chunk in bytes\n",
    "- **Delivery Rate:** rate (in bytes per second) at which data is delivered\n",
    "- **RRT (Round-Trip Time):** latency (in seconds) between client and server\n",
    "- **Transmission Time:**  total time taken to download the video chunk\n",
    "\n",
    "3. Please provide a high-level explanation of the model type that you plan to train / evaluate. Provide a link to the python sci-kit implementation that you plan to use.\n",
    "\n",
    "**We plan on using the *RandomForestRegressor* to train and evaluate our data collection. The *RandomForestRegressor* is suitable for our project because it can predict numerical outcomes, such as the download time for video chunks or QoE metrics. This can be useful in understanding the significance of different features in classifying network traffic. This scikit-learn model is more complex and useful for capturing non-linear relationships between features and outcomes. In the code below, we have provided the implementation of the model we plan on using for our project.**\n",
    "\n",
    "https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223320e",
   "metadata": {},
   "source": [
    "# From Checkpoint #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636d487",
   "metadata": {},
   "source": [
    "## EXPERIMENT\n",
    "\n",
    "*Provide a high-level explanation of the experiment(s) that you want to run through netUnicorn/netReplica. What type of data do you need to collect?*\n",
    "\n",
    "Our projectâ€™s goal is to predict download times of video chunks on video streaming platforms to minimize the interruptions caused by fluctuations in network performance. The input of our model will be historic QoS metrics, including throughput, latency, and packet loss. As for the training data we plan on running through netUnicorn/netReplica, we will collect time-series data on historical QoS metrics for video streaming sessions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1159a54",
   "metadata": {},
   "source": [
    "## DATA COLLECTION PIPELINES\n",
    "*Provide (pseudo)code for the pipeline(s) that you will run for your data collection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5b7b1",
   "metadata": {},
   "source": [
    "### Import Statements\n",
    "*For each task in your pipeline, provide a reference to the implementation of this task that you will use for your data collection.*\n",
    "\n",
    "These imports statements are our reference to the implementations used in our pipeline tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86096cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from netunicorn.client.remote import RemoteClient, RemoteClientException\n",
    "from netunicorn.base import Experiment, ExperimentStatus, Pipeline\n",
    "from netunicorn.library.tasks.capture.tcpdump import StartCapture, StopNamedCapture\n",
    "from netunicorn.library.tasks.upload.fileio import UploadToFileIO\n",
    "from netunicorn.library.tasks.upload.webdav import UploadToWebDav\n",
    "from netunicorn.library.tasks.basic import SleepTask\n",
    "from netunicorn.library.tasks.measurements.ookla_speedtest import SpeedTest\n",
    "from netunicorn.library.tasks.video_watchers.youtube_watcher import WatchYouTubeVideo\n",
    "from netunicorn.library.tasks.video_watchers.vimeo_watcher import WatchVimeoVideo\n",
    "from netunicorn.library.tasks.video_watchers.twitch_watcher import WatchTwitchStream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141631b",
   "metadata": {},
   "source": [
    "### Set Up netUnicorn\n",
    "Choosing a device for our project. Using our group's netUnicorn API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a050292",
   "metadata": {},
   "outputs": [],
   "source": [
    "NETUNICORN_ENDPOINT = os.environ.get('NETUNICORN_ENDPOINT', 'https://pinot.cs.ucsb.edu/netunicorn')\n",
    "NETUNICORN_LOGIN = os.environ.get('NETUNICORN_LOGIN', 'cs190n8')       \n",
    "NETUNICORN_PASSWORD = os.environ.get('NETUNICORN_PASSWORD', 'kfazTdrx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f860b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Check: True\n",
      "[<Uncountable node pool with next node template: [aws-fargate-A-cs190n8-, aws-fargate-B-cs190n8-, aws-fargate-ARM64-cs190n8-]>]\n"
     ]
    }
   ],
   "source": [
    "client = RemoteClient(endpoint=NETUNICORN_ENDPOINT, login=NETUNICORN_LOGIN, password=NETUNICORN_PASSWORD)\n",
    "print(\"Health Check: {}\".format(client.healthcheck()))\n",
    "nodes = client.get_nodes()\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb85fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_node = 'aws-fargate-A-cs190n8-'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b34d87",
   "metadata": {},
   "source": [
    "### Collecting Network Data for Video Streaming\n",
    "In our collecting network data pipeline we will be collecting packet captures while streaming video for YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40e22e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_533524/1177122223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetunicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_local_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_local_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from netunicorn.executor import get_local_executor\n",
    "executor = get_local_executor(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ac9228",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(426a5aff-b1f7-4afb-bc8a-a427446bdf46): {'root': [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd6bc820>], 1: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bc970>], 2: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bcb80>], 3: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bcbe0>], 4: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bcc10>], 5: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bcc70>], 6: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6bcb50>], 7: [<netunicorn.library.tasks.basic.SleepTask with name 01d1484f-a582-44b0-a190-16899b926e7a>], 8: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd6bcac0>], 9: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bce50>], 10: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bcf40>], 11: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bcf70>], 12: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bcfd0>], 13: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bd030>], 14: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6bcf10>], 15: [<netunicorn.library.tasks.basic.SleepTask with name 5fa84806-30d3-46a1-b908-21d3cb6bd906>], 16: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd6bce80>], 17: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bd210>], 18: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bff10>], 19: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bfee0>], 20: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bfe80>], 21: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bfe20>], 22: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6bff40>], 23: [<netunicorn.library.tasks.basic.SleepTask with name 78d6e0d4-f433-46f4-901f-69cc5f5b1b64>], 24: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd6bffd0>], 25: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6bfc70>], 26: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab310>], 27: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab340>], 28: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab3a0>], 29: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab400>], 30: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6ab2e0>], 31: [<netunicorn.library.tasks.basic.SleepTask with name a1d6ef4f-2520-4ef6-bc75-3a94b83d3f39>], 32: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd6ab250>], 33: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab5e0>], 34: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab6d0>], 35: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab700>], 36: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab760>], 37: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f50cd6ab7c0>], 38: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6ab6a0>], 39: [<netunicorn.library.tasks.basic.SleepTask with name 5d3bc492-eb41-4719-8e4d-e9fcaeef6c20>], 40: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd6ab610>], 41: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6ab9a0>], 42: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6aba90>], 43: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abaf0>], 44: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abb20>], 45: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abb80>], 46: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6aba60>], 47: [<netunicorn.library.tasks.basic.SleepTask with name 319618c0-0d2a-45e5-8b3d-0efb04d741fd>], 48: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd6ab9d0>], 49: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abd60>], 50: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abe50>], 51: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abe80>], 52: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abee0>], 53: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6abf40>], 54: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6abe20>], 55: [<netunicorn.library.tasks.basic.SleepTask with name 1c7e8120-0998-44f4-81ab-2536a6a1c285>], 56: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd613910>], 57: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd613820>], 58: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd613580>], 59: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd613550>], 60: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6131c0>], 61: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd613160>], 62: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd6135b0>], 63: [<netunicorn.library.tasks.basic.SleepTask with name 767f21e4-6d94-4926-84e9-e900fc1c5d31>], 64: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd613640>], 65: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612f80>], 66: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612e90>], 67: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612e60>], 68: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612e00>], 69: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612da0>], 70: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd612ec0>], 71: [<netunicorn.library.tasks.basic.SleepTask with name f029f116-16b8-4981-bd47-03c27e828033>], 72: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f50cd612f50>], 73: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612bc0>], 74: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612ad0>], 75: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612aa0>], 76: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd612a40>], 77: [<netunicorn.library.tasks.video_watchers.twitch_watcher.WatchTwitchStream object at 0x7f50cd6129e0>], 78: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f50cd612b00>], 79: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd612c50>], 80: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd612b90>], 81: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd6127a0>], 82: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd612800>], 83: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd612740>], 84: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd6121d0>], 85: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd612170>], 86: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd612110>], 87: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd6120b0>], 88: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f50cd612050>]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "\n",
    "# Flag to enable early stopping -- so if any task fails pipeline would go on working\n",
    "# pipeline.early_stopping = False\n",
    "\n",
    "# Generate Data for YouTube\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/youtube_1min.pcap\", name=\"1min_captureYT\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://www.youtube.com/watch?v=0g1Q4fBDp2U&pp=ygUMMSBtaW4gdmlkZW9z\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"1min_captureYT\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/youtube_2min.pcap\", name=\"2min_captureYT\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://www.youtube.com/watch?v=0CmtDk-joT4&ab_channel=Koi\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"2min_captureYT\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/youtube_5min.pcap\", name=\"5min_captureYT\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://www.youtube.com/watch?v=6-2ra25RVRs&pp=ygUMNSBtaW4gdmlkZW9z\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"5min_captureYT\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/youtube_10min.pcap\", name=\"10min_captureYT\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://www.youtube.com/watch?v=vCkhJeom7zU&pp=ygUNMTAgbWluIHZpZGVvcw%3D%3D\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"10min_captureYT\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/youtube_15min.pcap\", name=\"15min_captureYT\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://www.youtube.com/watch?v=co47u19cbds&pp=ygUNMTUgbWluIHZpZGVvcw%3D%3D\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"15min_captureYT\"))\n",
    "\n",
    "# Generate Data for Twitch\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/twitch_capture1.pcap\", name=\"1_captureTW\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchTwitchStream(\"https://www.twitch.tv/emilycc/clip/BoringGloriousGoblinOMGScoots-HzN323BdbgWMC8z2\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"1_captureTW\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/twitch_capture2.pcap\", name=\"2_captureTW\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchTwitchStream(\"https://www.twitch.tv/chess24/clip/EvilHedonisticLadiesDBstyle-Qvx2Iv5rD18TBkMc\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"2_captureTW\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/twitch_capture3.pcap\", name=\"3_captureTW\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchTwitchStream(\"https://www.twitch.tv/dinabelenkaya/clip/BlueArtisticLarkDendiFace-6GsaEv3Rt8vpRoTL\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"3_captureTW\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/twitch_capture4.pcap\", name=\"4_captureTW\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchTwitchStream(\"https://www.twitch.tv/chess24/clip/PlayfulEndearingOpossumPeoplesChamp-d1sejwuAMspNlEz7\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"4_captureTW\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/twitch_capture5.pcap\", name=\"5_captureTW\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchTwitchStream(\"\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"5_captureTW\"))\n",
    "\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/youtube_1min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/youtube_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/youtube_2min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/youtube_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/youtube_5min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/youtube_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/youtube_10min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/youtube_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/youtube_15min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/youtube_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/twitch_capture1.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/twitch_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/twitch_capture2.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/twitch_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/twitch_capture3.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/twitch_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/twitch_capture4.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/twitch_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/twitch_capture5.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/twitch_capture\", username=\"uploader\", password=\"uploader\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7138dd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:executor_local:asyncio.run() cannot be called from a running event loop\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/netunicorn/netunicorn-executor/src/netunicorn/executor/executor.py\", line 134, in __call__\n",
      "    asyncio.run(self.execute())\n",
      "  File \"/usr/lib/python3.10/asyncio/runners.py\", line 33, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: asyncio.run() cannot be called from a running event loop\n",
      "CRITICAL:executor_local:Failed to execute the graph. Shutting down.\n",
      "/srv/netunicorn/netunicorn-executor/src/netunicorn/executor/executor.py:143: RuntimeWarning: coroutine 'Executor.execute' was never awaited\n",
      "  break\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "INFO:executor_local:Skipping reporting results due to execution graph setting.\n"
     ]
    }
   ],
   "source": [
    "executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9131a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(94e758bd-c3e4-415d-9ae0-3898c9c0a863): {'root': [<netunicorn.library.tasks.basic.SleepTask with name 886ff3c3-8d68-4152-afcf-3cfe33228a45>], 1: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f38c9e8f700>], 2: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e8f6d0>], 3: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e8c880>], 4: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e8c7c0>], 5: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e8c820>], 6: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e8c7f0>], 7: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f38c9e8c700>], 8: [<netunicorn.library.tasks.basic.SleepTask with name 2bd9699a-e514-46a9-9728-db5da4df4ac2>], 9: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f38c9e8ec20>], 10: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e8c8b0>], 11: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e2fa60>], 12: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e2fa30>], 13: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e2ea10>], 14: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e2eda0>], 15: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f38c9e2fa90>], 16: [<netunicorn.library.tasks.basic.SleepTask with name 29fc15ce-e0bf-4968-9935-4e24c8aae823>], 17: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f38c9e2ece0>], 18: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9e2c760>], 19: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f399721e5c0>], 20: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f399721e9e0>], 21: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f399721d5d0>], 22: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f399721f010>], 23: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f38c9e2e560>], 24: [<netunicorn.library.tasks.basic.SleepTask with name 60260ecc-1e98-44ec-bbb6-0116c6f46676>], 25: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f399721c940>], 26: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0a30>], 27: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0940>], 28: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0910>], 29: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed08b0>], 30: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0850>], 31: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f38c9ed0970>], 32: [<netunicorn.library.tasks.basic.SleepTask with name a2e47b97-ad1d-46a6-810d-20c8dbb3f0c2>], 33: [<netunicorn.library.tasks.capture.tcpdump.StartCapture object at 0x7f38c9ed0a00>], 34: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0670>], 35: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0580>], 36: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0550>], 37: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed04f0>], 38: [<netunicorn.library.tasks.video_watchers.vimeo_watcher.WatchVimeoVideo object at 0x7f38c9ed0490>], 39: [<netunicorn.library.tasks.capture.tcpdump.StopNamedCapture object at 0x7f38c9ed05b0>], 40: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f38c9ed0700>], 41: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f38c9ed0640>], 42: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f38c9ed02b0>], 43: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f38c9ed0250>], 44: [<netunicorn.library.tasks.upload.webdav.UploadToWebDav object at 0x7f38c9ed01f0>]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "# Flag to enable early stopping -- so if any task fails pipeline would go on working\n",
    "# pipeline.early_stopping = False\n",
    "\n",
    "# Generate Data for Vimeo\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/vimeo_1min.pcap\", name=\"1min_captureVM\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://vimeo.com/820625227?autoplay=1\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"1min_captureVM\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/vimeo_2min.pcap\", name=\"2min_captureVM\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://vimeo.com/867196026\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"2min_captureVM\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/vimeo_5min.pcap\", name=\"5min_captureVM\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://vimeo.com/391703912\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"5min_captureVM\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/vimeo_10min.pcap\", name=\"10min_captureVM\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://vimeo.com/675873896\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"10min_captureVM\"))\n",
    "\n",
    "pipeline.then(SleepTask(2))\n",
    "\n",
    "pipeline.then(StartCapture(filepath=\"/tmp/vimeo_15min.pcap\", name=\"15min_captureVM\"))\n",
    "for _ in range(5):\n",
    "    pipeline.then(WatchVimeoVideo(\"https://vimeo.com/1031379349\", 10))\n",
    "pipeline.then(StopNamedCapture(start_capture_task_name=\"15min_captureVM\"))\n",
    "\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/vimeo_1min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/vimeo_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/vimeo_2min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/vimeo_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/vimeo_5min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/vimeo_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/vimeo_10min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/vimeo_capture\", username=\"uploader\", password=\"uploader\"))\n",
    "pipeline.then(UploadToWebDav(filepaths={\"/tmp/vimeo_15min.pcap\"}, endpoint=\"http://snl-server-5.cs.ucsb.edu/cs190n/cs190n8/vimeo_capture\", username=\"uploader\", password=\"uploader\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1040e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Check: True\n",
      "[<Uncountable node pool with next node template: [aws-fargate-A-cs190n8-, aws-fargate-B-cs190n8-, aws-fargate-ARM64-cs190n8-]>]\n"
     ]
    }
   ],
   "source": [
    "client = RemoteClient(endpoint=NETUNICORN_ENDPOINT, login=NETUNICORN_LOGIN, password=NETUNICORN_PASSWORD)\n",
    "print(\"Health Check: {}\".format(client.healthcheck()))\n",
    "nodes = client.get_nodes()\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138d51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Deployment: Node=aws-fargate-A-cs190n8-1, executor_id=, prepared=False, error=None\n"
     ]
    }
   ],
   "source": [
    "working_nodes = nodes.filter(lambda node: node.name.startswith(working_node)).take(1)\n",
    "\n",
    "# Creating the experiment\n",
    "experiment = Experiment().map(pipeline, working_nodes)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6675d33",
   "metadata": {},
   "source": [
    "### Preparing the Experiment\n",
    "We will use a predefined DockerImage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad1d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netunicorn.base import DockerImage\n",
    "for deployment in experiment:\n",
    "    # you can explore the image on the DockerHub\n",
    "    deployment.environment_definition = DockerImage(image='satyandraguthula/netunicorn_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b2f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_label = \"da1aco113c1i0ns\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc693e",
   "metadata": {},
   "source": [
    "Now we can prepare the experiment, check for any errors and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23eab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperimentStatus.PREPARING\n",
      "ExperimentStatus.READY\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client.delete_experiment(experiment_label)\n",
    "except RemoteClientException:\n",
    "    pass\n",
    "\n",
    "client.prepare_experiment(experiment, experiment_label)\n",
    "\n",
    "while True:\n",
    "    info = client.get_experiment_status(experiment_label)\n",
    "    print(info.status)\n",
    "    if info.status == ExperimentStatus.READY:\n",
    "        break\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd2fb40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared: True, error: None\n"
     ]
    }
   ],
   "source": [
    "for deployment in client.get_experiment_status(experiment_label).experiment:\n",
    "    print(f\"Prepared: {deployment.prepared}, error: {deployment.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b593b700",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.FINISHED\n"
     ]
    }
   ],
   "source": [
    "client.start_execution(experiment_label)\n",
    "\n",
    "while True:\n",
    "    info = client.get_experiment_status(experiment_label)\n",
    "    print(info.status)\n",
    "    if info.status != ExperimentStatus.RUNNING:\n",
    "        break\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c153d9a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node name: aws-fargate-A-cs190n8-1\n",
      "Error: None\n",
      "Result is: <class 'returns.result.Failure'>\n",
      "1min_captureYT: [<Success: 9>]\n",
      "3b6eff90-078b-45e1-9c71-57619891588f: [<Success: Video finished by timeout: 10 seconds>]\n",
      "65cb8823-f035-4133-b040-66721a8b6637: [<Success: Video finished by timeout: 10 seconds>]\n",
      "43e4e13b-36a3-47d6-92b6-0ce438ddf593: [<Success: Video finished by timeout: 10 seconds>]\n",
      "61e0c191-a261-4473-8f54-53acddac555c: [<Failure: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/netunicorn/executor/utils.py\", line 32, in decorator\n",
      "    result = function(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/netunicorn/library/tasks/video_watchers/vimeo_watcher.py\", line 134, in run\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/netunicorn/library/tasks/video_watchers/vimeo_watcher.py\", line 74, in watch\n",
      "  File \"/usr/local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py\", line 495, in close\n",
      "    self.execute(Command.CLOSE)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py\", line 384, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py\", line 232, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.TimeoutException: Message: timeout: Timed out receiving message from renderer: 20.000\n",
      "  (Session info: chrome=125.0.6422.141)\n",
      "Stacktrace:\n",
      "#0 0x55b21ffdae3a <unknown>\n",
      "#1 0x55b21fcc445c <unknown>\n",
      "#2 0x55b21fcab816 <unknown>\n",
      "#3 0x55b21fca9336 <unknown>\n",
      "#4 0x55b21fca99af <unknown>\n",
      "#5 0x55b21fcc49c6 <unknown>\n",
      "#6 0x55b21fc9d94c <unknown>\n",
      "#7 0x55b21fc9db9b <unknown>\n",
      "#8 0x55b21fd42e5d <unknown>\n",
      "#9 0x55b21fd33478 <unknown>\n",
      "#10 0x55b21fd031c7 <unknown>\n",
      "#11 0x55b21fd03b3e <unknown>\n",
      "#12 0x55b21ffa127b <unknown>\n",
      "#13 0x55b21ffa5327 <unknown>\n",
      "#14 0x55b21ff8ddae <unknown>\n",
      "#15 0x55b21ffa5df2 <unknown>\n",
      "#16 0x55b21ff7274f <unknown>\n",
      "#17 0x55b21ffca128 <unknown>\n",
      "#18 0x55b21ffca2fb <unknown>\n",
      "#19 0x55b21ffd9f6c <unknown>\n",
      "#20 0x7f2d6dd3f1c4 <unknown>\n",
      "\n",
      ">]\n",
      "Parsed configuration: Gateway located on https://pinot.cs.ucsb.edu/netunicorn/gateway\n",
      "Current directory: /\n",
      "Successfully received the execution graph.\n",
      "Traceback (most recent call last):\n",
      "File \"/usr/local/lib/python3.10/site-packages/netunicorn/executor/utils.py\", line 32, in decorator\n",
      "result = function(*args, **kwargs)\n",
      "File \"/usr/local/lib/python3.10/dist-packages/netunicorn/library/tasks/video_watchers/vimeo_watcher.py\", line 134, in run\n",
      "File \"/usr/local/lib/python3.10/dist-packages/netunicorn/library/tasks/video_watchers/vimeo_watcher.py\", line 74, in watch\n",
      "File \"/usr/local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py\", line 495, in close\n",
      "self.execute(Command.CLOSE)\n",
      "File \"/usr/local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py\", line 384, in execute\n",
      "self.error_handler.check_response(response)\n",
      "File \"/usr/local/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py\", line 232, in check_response\n",
      "raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.TimeoutException: Message: timeout: Timed out receiving message from renderer: 20.000\n",
      "(Session info: chrome=125.0.6422.141)\n",
      "Stacktrace:\n",
      "#0 0x55b21ffdae3a <unknown>\n",
      "#1 0x55b21fcc445c <unknown>\n",
      "#2 0x55b21fcab816 <unknown>\n",
      "#3 0x55b21fca9336 <unknown>\n",
      "#4 0x55b21fca99af <unknown>\n",
      "#5 0x55b21fcc49c6 <unknown>\n",
      "#6 0x55b21fc9d94c <unknown>\n",
      "#7 0x55b21fc9db9b <unknown>\n",
      "#8 0x55b21fd42e5d <unknown>\n",
      "#9 0x55b21fd33478 <unknown>\n",
      "#10 0x55b21fd031c7 <unknown>\n",
      "#11 0x55b21fd03b3e <unknown>\n",
      "#12 0x55b21ffa127b <unknown>\n",
      "#13 0x55b21ffa5327 <unknown>\n",
      "#14 0x55b21ff8ddae <unknown>\n",
      "#15 0x55b21ffa5df2 <unknown>\n",
      "#16 0x55b21ff7274f <unknown>\n",
      "#17 0x55b21ffca128 <unknown>\n",
      "#18 0x55b21ffca2fb <unknown>\n",
      "#19 0x55b21ffd9f6c <unknown>\n",
      "#20 0x7f2d6dd3f1c4 <unknown>\n",
      "\n",
      "Execution is finished, start reporting results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from returns.pipeline import is_successful\n",
    "\n",
    "for report in info.execution_result:\n",
    "    print(f\"Node name: {report.node.name}\")\n",
    "    print(f\"Error: {report.error}\")\n",
    "\n",
    "    result, log = report.result  # report stores results of execution and corresponding log\n",
    "    \n",
    "    # result is a returns.result.Result object, could be Success of Failure\n",
    "    print(f\"Result is: {type(result)}\")\n",
    "    data = result.unwrap() if is_successful(result) else result.failure()\n",
    "    for key, value in data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # we also can explore logs\n",
    "    for line in log:\n",
    "        print(line.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95159e9",
   "metadata": {},
   "source": [
    "# Checkpoint #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed019ea0",
   "metadata": {},
   "source": [
    "### Convert Packet Trace to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "682cf746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/md0/cs190n-test/youtube_video_capture_ISCX.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4170512/3454718174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_youtube\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/md0/cs190n-test/youtube_video_capture_ISCX.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_vimeo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/md0/cs190n-test/vimeo_video_capture_ISCX.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_twitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/md0/cs190n/cs190n8/twitch_stream_capture_ISCX.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_youtube\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/md0/cs190n-test/youtube_video_capture_ISCX.csv'"
     ]
    }
   ],
   "source": [
    "df_youtube1 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/youtube_1min_capture_ISCX.csv\")\n",
    "df_youtube2 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/youtube_2min_capture_ISCX.csv\")\n",
    "df_youtube5 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/youtube_5min_capture_ISCX.csv\")\n",
    "df_youtube10 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/youtube_10min_capture_ISCX.csv\")\n",
    "df_youtube15 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/youtube_15min_capture_ISCX.csv\")\n",
    "\n",
    "df_vimeo1 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/vimeo_1min_capture_ISCX.csv\")\n",
    "df_vimeo2 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/vimeo_2min_capture_ISCX.csv\")\n",
    "df_vimeo5 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/vimeo_5min_capture_ISCX.csv\")\n",
    "df_vimeo10 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/vimeo_10min_capture_ISCX.csv\")\n",
    "df_vimeo15 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/vimeo_15min_capture_ISCX.csv\")\n",
    "\n",
    "df_twitch1 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/twitch_capture1_ISCX.csv\")\n",
    "df_twitch2 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/twitch_capture2_ISCX.csv\")\n",
    "df_twitch3 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/twitch_capture3_ISCX.csv\")\n",
    "df_twitch4 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/twitch_capture4_ISCX.csv\")\n",
    "df_twitch5 = pd.read_csv(\"/mnt/md0/cs190n/cs190n8/twitch_capture5_ISCX.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bd36b",
   "metadata": {},
   "source": [
    "### Separate CSVs to video_sent & video_acked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35428b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_twitch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193969/2742352043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_twitch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_twitch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_twitch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total Fwd Packet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_twitch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total Bwd packets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'twitch'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_twitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_twitch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_twitch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_twitch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Protocol'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_twitch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'twitch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_twitch' is not defined"
     ]
    }
   ],
   "source": [
    "def separate_video_packets(df):\n",
    "    # Add label to classify packets as 'sent' or 'acked'\n",
    "    df['Label'] = 'other'\n",
    "    \n",
    "    # Define criteria for video sent and video acked\n",
    "    df.loc[df['Total Fwd Packet'] > 0, 'Label'] = 'sent'  # Forward packets (video sent)\n",
    "    df.loc[df['Total Bwd packets'] > 0, 'Label'] = 'acked'  # Backward packets (video acked)\n",
    "    \n",
    "    # Filter out irrelevant rows (optional, depending on your needs)\n",
    "    df = df.drop(df[(df['Protocol'] == 17) & (df['Label'] != 'sent')].index)\n",
    "    \n",
    "    # Separate into sent and acked dataframes\n",
    "    df_sent = df[df['Label'] == 'sent']\n",
    "    df_acked = df[df['Label'] == 'acked']\n",
    "    \n",
    "    return df_sent, df_acked\n",
    "\n",
    "df_youtube1_sent, df_youtube1_acked = separate_video_packets(df_youtube1)\n",
    "df_youtube2_sent, df_youtube2_acked = separate_video_packets(df_youtube2)\n",
    "df_youtube5_sent, df_youtube5_acked = separate_video_packets(df_youtube5)\n",
    "df_youtube10_sent, df_youtube10_acked = separate_video_packets(df_youtube10)\n",
    "df_youtube15_sent, df_youtube15_acked = separate_video_packets(df_youtube15)\n",
    "\n",
    "df_vimeo1_sent, df_vimeo1_acked = separate_video_packets(df_vimeo1)\n",
    "df_vimeo2_sent, df_vimeo2_acked = separate_video_packets(df_vimeo2)\n",
    "df_vimeo5_sent, df_vimeo5_acked = separate_video_packets(df_vimeo5)\n",
    "df_vimeo10_sent, df_vimeo10_acked = separate_video_packets(df_vimeo10)\n",
    "df_vimeo15_sent, df_vimeo15_acked = separate_video_packets(df_vimeo15)\n",
    "\n",
    "df_twitch1_sent, df_twitch1_acked = separate_video_packets(df_twitch1)\n",
    "df_twitch2_sent, df_twitch2_acked = separate_video_packets(df_twitch2)\n",
    "df_twitch3_sent, df_twitch3_acked = separate_video_packets(df_twitch3)\n",
    "df_twitch4_sent, df_twitch4_acked = separate_video_packets(df_twitch4)\n",
    "df_twitch5_sent, df_twitch5_acked = separate_video_packets(df_twitch5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DURATION = 180180\n",
    "PKT_BYTES = 1500\n",
    "MILLION = 1000000\n",
    "PAST_CHUNKS = 8\n",
    "FUTURE_CHUNKS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c11a87",
   "metadata": {},
   "source": [
    "### Prepare CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data(video_sent_path, video_acked_path, time_start=None, time_end=None):\n",
    "    \"\"\"\n",
    "    Load data from files and calculate chunk transmission times.\n",
    "    \"\"\"\n",
    "    video_sent_df = pd.read_csv(video_sent_path)\n",
    "    video_acked_df = pd.read_csv(video_acked_path)\n",
    "\n",
    "    # Rename \"time (ns GMT)\" to \"time\" for convenience\n",
    "    video_sent_df.rename(columns={'time (ns GMT)': 'time'}, inplace=True)\n",
    "    video_acked_df.rename(columns={'time (ns GMT)': 'time'}, inplace=True)\n",
    "\n",
    "    # Convert nanosecond timestamps to datetime\n",
    "    video_sent_df['time'] = pd.to_datetime(video_sent_df['time'], unit='ns')\n",
    "    video_acked_df['time'] = pd.to_datetime(video_acked_df['time'], unit='ns')\n",
    "\n",
    "    # Filter by time range\n",
    "    if time_start:\n",
    "        time_start = pd.to_datetime(time_start)\n",
    "        video_sent_df = video_sent_df[video_sent_df['time'] >= time_start]\n",
    "        video_acked_df = video_acked_df[video_acked_df['time'] >= time_start]\n",
    "    if time_end:\n",
    "        time_end = pd.to_datetime(time_end)\n",
    "        video_sent_df = video_sent_df[video_sent_df['time'] <= time_end]\n",
    "        video_acked_df = video_acked_df[video_acked_df['time'] <= time_end]\n",
    "\n",
    "    # Process the data\n",
    "    return calculate_trans_times(video_sent_df, video_acked_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ae1b1",
   "metadata": {},
   "source": [
    "### Calculate Transmission Times & Divide into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52749a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trans_times(video_sent_df, video_acked_df):\n",
    "    \"\"\"\n",
    "    Calculate transmission times from video_sent and video_acked datasets using session_id.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    last_video_ts = {}\n",
    "\n",
    "    for _, row in video_sent_df.iterrows():\n",
    "        session = row['session_id']  # Use only session_id to track sessions\n",
    "        if session not in d:\n",
    "            d[session] = {}\n",
    "            last_video_ts[session] = None\n",
    "\n",
    "        video_ts = int(row['video_ts'])\n",
    "        if last_video_ts[session] is not None:\n",
    "            if video_ts != last_video_ts[session] + VIDEO_DURATION:\n",
    "                continue\n",
    "\n",
    "        last_video_ts[session] = video_ts\n",
    "        d[session][video_ts] = {\n",
    "            'sent_ts': pd.Timestamp(row['time']),\n",
    "            'size': float(row['size']) / PKT_BYTES,\n",
    "            'delivery_rate': float(row['delivery_rate']) / PKT_BYTES,\n",
    "            'rtt': float(row['rtt']) / MILLION,\n",
    "        }\n",
    "\n",
    "    for _, row in video_acked_df.iterrows():\n",
    "        session = row['session_id']  # Use only session_id\n",
    "        if session not in d:\n",
    "            continue\n",
    "\n",
    "        video_ts = int(row['video_ts'])\n",
    "        if video_ts not in d[session]:\n",
    "            continue\n",
    "\n",
    "        dsv = d[session][video_ts]\n",
    "        sent_ts = dsv['sent_ts']\n",
    "        acked_ts = pd.Timestamp(row['time'])\n",
    "        dsv['acked_ts'] = acked_ts\n",
    "        dsv['trans_time'] = (acked_ts - sent_ts).total_seconds()\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_raw_data(\"/mnt/md0/cs190n/video_sent.csv\", \"/mnt/md0/cs190n/video_acked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_past_chunks(ds, next_ts, row):\n",
    "    i = 1\n",
    "    past_chunks = []\n",
    "    while i <= PAST_CHUNKS:\n",
    "        ts = next_ts - i * VIDEO_DURATION\n",
    "        if ts in ds and 'trans_time' in ds[ts]:\n",
    "            past_chunks = [ds[ts]['delivery_rate'],\n",
    "                           ds[ts]['rtt'],\n",
    "                           ds[ts]['size'], \n",
    "                           ds[ts]['trans_time']] + past_chunks\n",
    "        else:\n",
    "            nts = ts + VIDEO_DURATION  # padding with the nearest ts\n",
    "            padding = [ds[nts]['delivery_rate'],\n",
    "                       ds[nts]['rtt']]\n",
    "            if nts == next_ts:\n",
    "                padding += [0, 0]  # next_ts is the first chunk to send\n",
    "            else:\n",
    "                padding += [ds[nts]['size'], ds[nts]['trans_time']]\n",
    "            break\n",
    "        i += 1\n",
    "    if i != PAST_CHUNKS + 1:  # break in the middle; padding must exist\n",
    "        while i <= PAST_CHUNKS:\n",
    "            past_chunks = padding + past_chunks\n",
    "            i += 1\n",
    "    row += past_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_output(d):\n",
    "    ret = [{'in': [], 'out': []} for _ in range(5)]  # FUTURE_CHUNKS = 5\n",
    "\n",
    "    for session in d:\n",
    "        ds = d[session]\n",
    "\n",
    "        for next_ts in ds:\n",
    "            if 'trans_time' not in ds[next_ts]:\n",
    "                continue\n",
    "\n",
    "            row = []\n",
    "\n",
    "            # Append past chunks\n",
    "            append_past_chunks(ds, next_ts, row)\n",
    "\n",
    "            # Append the TCP info of the next chunk\n",
    "            row += [ds[next_ts]['delivery_rate'],\n",
    "                    ds[next_ts]['rtt']]\n",
    "\n",
    "            # Generate FUTURE_CHUNKS rows\n",
    "            for i in range(5):  # FUTURE_CHUNKS = 5\n",
    "                row_i = row.copy()\n",
    "\n",
    "                ts = next_ts + i * VIDEO_DURATION\n",
    "                if ts in ds and 'trans_time' in ds[ts]:\n",
    "                    row_i += [ds[ts]['size']]\n",
    "\n",
    "                    ret[i]['in'].append(row_i)\n",
    "                    ret[i]['out'].append(ds[ts]['trans_time'])\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667864a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(output_file, processed_data):\n",
    "    \"\"\"\n",
    "    Save processed data to a file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "    print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7dd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    DEFAULT_VIDEO_SENT_PATH = ''\n",
    "    DEFAULT_VIDEO_ACKED_PATH = ''\n",
    "    DEFAULT_OUTPUT_FILE = ''\n",
    "    \n",
    "    #Latest datasets can be found at https://puffer.stanford.edu/results/\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Process video streaming datasets.\")\n",
    "    parser.add_argument('--video_sent_path', type=str, help='Path to the video_sent dataset CSV file.')\n",
    "    parser.add_argument('--video_acked_path', type=str, help='Path to the video_acked dataset CSV file.')\n",
    "    parser.add_argument('--output_file', type=str, help='Path to save the processed data.')\n",
    "    parser.add_argument('--time_start', type=str, default=None, help='Start time for filtering data (RFC3339 format).')\n",
    "    parser.add_argument('--time_end', type=str, default=None, help='End time for filtering data (RFC3339 format).')\n",
    "    #args = parser.parse_args()\n",
    "    #processed_data = prepare_input_output(prepare_raw_data(args.video_sent_path, args.video_acked_path,\n",
    "    #    time_start=args.time_start, time_end=args.time_end))\n",
    "    # save_processed_data(args.output_file, processed_data)\n",
    "    processed_data = prepare_input_output(prepare_raw_data(DEFAULT_VIDEO_SENT_PATH, DEFAULT_VIDEO_ACKED_PATH,\n",
    "        time_start=None, time_end=None))\n",
    "    save_processed_data(DEFAULT_OUTPUT_FILE, processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25481b",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3368f7eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/psabherwal/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/psabherwal/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/psabherwal/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8add01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features and target variable\n",
    "X = df[['trans_time', 'size', 'delivery_rate', 'rtt']]  \n",
    "y = df['video_chunk_download_time']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# R-squared score (good for understanding how well the model fits the data)\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# Feature Importance: This shows the relative importance of each feature\n",
    "print(\"Feature Importances:\")\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c9a98",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Actual vs Predicted (Random Forest)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
